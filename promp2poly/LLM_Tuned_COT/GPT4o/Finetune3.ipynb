{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c158105-d773-4ef9-8ce5-281805703ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client=OpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8dbe4ef-cc21-4645-8dc9-9f7b36b09aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_small_all ='Data/lat/training_small_combined_conversational.jsonl'\n",
    "validation_file_small_all = 'Data/lat/validation_small_combined_conversational.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "715501ac-9fad-49cb-8e4f-d7c5b263b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-2NyDtDWkUquQzN2STSSqct', bytes=2074238, created_at=1744298505, filename='training_small_combined_conversational.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)\n",
      "---------------------------------\n",
      "FileObject(id='file-ByHuXxJEayPLcRNmiE7WnA', bytes=231982, created_at=1744298506, filename='validation_small_combined_conversational.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)\n"
     ]
    }
   ],
   "source": [
    "training_file_small_all=client.files.create(file=open(training_file_small_all,'rb'),\n",
    "                    purpose='fine-tune')\n",
    "\n",
    "validation_file_small_all=client.files.create(file=open(validation_file_small_all,'rb'),\n",
    "                    purpose='fine-tune')\n",
    "print(training_file_small_all)\n",
    "print('---------------------------------')\n",
    "print(validation_file_small_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df3ee4a8-513a-4956-999b-db426d944cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-9rdJVUwuLvnSv0f7aEFD4YJL', created_at=1744298508, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-AVOdG0yGREZO41elnVjY6u7y', result_files=[], seed=766645144, status='validating_files', trained_tokens=None, training_file='file-2NyDtDWkUquQzN2STSSqct', validation_file='file-ByHuXxJEayPLcRNmiE7WnA', estimated_finish=None, integrations=[], user_provided_suffix=None, metadata=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'batch_size': 'auto', 'learning_rate_multiplier': 'auto', 'n_epochs': 'auto'}}})\n"
     ]
    }
   ],
   "source": [
    "response_small_all=client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_small_all.id,\n",
    "    validation_file=validation_file_small_all.id,\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    hyperparameters={\n",
    "    'n_epochs':'auto',\n",
    "    'batch_size':'auto',  \n",
    "})\n",
    "\n",
    "print(response_small_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a06035-2076-4b55-8440-e032d601df36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f9d1813-179f-4da9-b19d-22b659815a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-9rdJVUwuLvnSv0f7aEFD4YJL', created_at=1744298508, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI', finished_at=1744300923, hyperparameters=Hyperparameters(n_epochs=3, batch_size=5, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-AVOdG0yGREZO41elnVjY6u7y', result_files=['file-2N78z8wM7npZDAJ944ntnE'], seed=766645144, status='succeeded', trained_tokens=1638468, training_file='file-2NyDtDWkUquQzN2STSSqct', validation_file='file-ByHuXxJEayPLcRNmiE7WnA', estimated_finish=None, integrations=[], user_provided_suffix=None, metadata=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'n_epochs': 3, 'batch_size': 5, 'learning_rate_multiplier': 1.8}}})\n"
     ]
    }
   ],
   "source": [
    "ft_model_small_all=client.fine_tuning.jobs.retrieve(response_small_all.id)#(response.id)\n",
    "print(ft_model_small_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9db7c26b-bb9b-40c6-a2bc-0f06ea99b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a polymer expert specializing in thermoset shape memory polymers. Your role is to analyze and create monomer pairs with excellent thermal stability and mechanical strength.\"\n",
    "user_message_1 =['I want to make a thermoset shape memory polymer','Please suggest me some TSMPs']\n",
    "user_message_2 =['Please focus on property based monomer pairs','Please focus on group based monomer pairs']\n",
    "proeprty_specific_message = [\"Please give me some TSMP with Tg = 100C and Er= 150MPa\",\"Please generate some TSMP with Tg = 50C and Er= 100Mpa\"]\n",
    "group_specific_message = [\"Please give me some TSMP with epoxy(C1OC1) groups in monomer 1 and imine(NC) groups in monomer 2\",\"Please generate some TSMP with vinyl(C=C) groups in monomer 1 and thiol(CCS) groups in monomer 2\"]\n",
    "mixed_specific_message = [\"Please give me some TSMP with Tg = 100C and Er= 150MPa and vinyl(C=C) groups in monomer 1 and vinyl(C=C) groups in monomer 2\",\"Please generate some TSMP with Tg = 50C and Er= 100Mpa and Thiol(CCS) groups in monomer 1 and vinyl(C=C) groups in monomer 2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17e66aae-2150-4e41-afdd-03a9b562547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  I want to make a thermoset shape memory polymer\n",
      "Assistant:  What's your priority for the TSMP - working with specific groups or meeting certain property targets?\n",
      "User:  Please focus on property based monomer pairs\n",
      "Assistant:  I'll need two key properties to design your TSMP: the glass transition temperature (Tg) and stress recovery (Er). What values would you like?\n",
      "User:  Please give me some TSMP with Tg = 100C and Er= 150MPa\n",
      "Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \n",
      "Sample : : 1 Great choice of properties and groups. Here's a compatible monomer pair:\n",
      "Monomer 1: C=C(C)C(=O)OC\n",
      "Monomer 2: C=C(C)C(=O)OCCOCCOCCOCCOCCOCCOCCOCCOCCOC(=O)C(=C)C\n",
      "Sample : : 2 Based on your requirements, I suggest the following monomers:\n",
      "Monomer 1: c3cc(N(CC1CO1)CC2CO2)ccc3OCC4CO4\n",
      "Monomer 2: NCCNCCN(CCNCCC(CN)CCN)CCN(CCNCCN)CCN(CCN)CCN\n"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "def generate_new_TSMP(role,prompt_content, isFinalQuery=False):\n",
    "    propmt={\"role\":role, \"content\": prompt_content}\n",
    "    messages.append(propmt)\n",
    "    if not isFinalQuery:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=200,\n",
    "            n=2\n",
    "        )\n",
    "        result= completion\n",
    "       \n",
    "        \n",
    "   \n",
    "    messages.append({\"role\":'assistant', \"content\": result})\n",
    "    \n",
    "    # Print the response from the assistant\n",
    "    return result, messages\n",
    "\n",
    "\n",
    "replies_0, messages_0 = generate_new_TSMP('user',user_message_1[0], isFinalQuery=False)\n",
    "print(\"User: \",user_message_1[0])\n",
    "print(\"Assistant: \",replies_0)\n",
    "replies_1, messages_1 = generate_new_TSMP('user',user_message_2[0], isFinalQuery=False)\n",
    "print(\"User: \",user_message_2[0])\n",
    "print(\"Assistant: \",replies_1)\n",
    "replies_2, messages_2 = generate_new_TSMP('user',proeprty_specific_message[0], isFinalQuery=True)\n",
    "print(\"User: \",proeprty_specific_message[0])\n",
    "print(\"Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \")\n",
    "for i in range(2):\n",
    "    print(\"Sample : :\",i+1, replies_2.choices[i].message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67e324fe-d832-45a2-ad21-37510b618f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Please suggest me some TSMPs\n",
      "Assistant:  Would you like to design your TSMP based on specific functional groups or target properties?\n",
      "User:  Please focus on group based monomer pairs\n",
      "Assistant:  Please specify your required functional groups - I need one for one monomer and another for the second.\n",
      "User:  Please give me some TSMP with epoxy(C1OC1) groups in monomer 1 and imine(NC) groups in monomer 2\n",
      "Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \n",
      "Sample : : 1 Great choice of groups. Here's a compatible monomer pair:\n",
      "Monomer 1: Cc3cc(c2cc(C)c(OCC1CO1)c(C)c2)cc(C)c3OCC4CO4\n",
      "Monomer 2: CC1(C)CC(N)CC(C)(CN)C1\n",
      "Sample : : 2 Based on your requirements, I suggest the following monomers:\n",
      "Monomer 1 with C1OC1 groups: CC(C)(c2ccc(OC1CO1)cc2)c4ccc(OC3CO3)cc4\n",
      "Monomer 2 with NC groups: NCCCOCCOCCCN\n"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "def generate_new_TSMP(role,prompt_content, isFinalQuery=False):\n",
    "    propmt={\"role\":role, \"content\": prompt_content}\n",
    "    messages.append(propmt)\n",
    "    if not isFinalQuery:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=200,\n",
    "            n=2\n",
    "        )\n",
    "        result= completion\n",
    "       \n",
    "        \n",
    "   \n",
    "    messages.append({\"role\":'assistant', \"content\": result})\n",
    "    \n",
    "    # Print the response from the assistant\n",
    "    return result, messages\n",
    "\n",
    "replies_0, messages_0 = generate_new_TSMP('user',user_message_1[1], isFinalQuery=False)\n",
    "print(\"User: \",user_message_1[1])\n",
    "print(\"Assistant: \",replies_0)\n",
    "replies_1, messages_1 = generate_new_TSMP('user',user_message_2[1], isFinalQuery=False)\n",
    "print(\"User: \",user_message_2[1])\n",
    "print(\"Assistant: \",replies_1)\n",
    "replies_2, messages_2 = generate_new_TSMP('user',group_specific_message[0], isFinalQuery=True)\n",
    "print(\"User: \",group_specific_message[0])\n",
    "print(\"Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \")\n",
    "for i in range(2):\n",
    "    print(\"Sample : :\",i+1, replies_2.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7026283a-4c0b-4135-9720-106e98a988b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Please generate some TSMP with vinyl(C=C) groups in monomer 1 and thiol(CCS) groups in monomer 2\n",
      "Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \n",
      "Sample : : 1 These monomers match the required group profile:\n",
      "- Monomer 1 has C=C groups: C=C(C)C(=O)OCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOC(=O)C(=C)C\n",
      "- Monomer 2 has CCS groups: O=c1n(CCS)c(=O)n(CCS)c(=O)n1CCS\n",
      "Sample : : 2 Here is a group-compliant monomer pair:\n",
      "Monomer 1 includes C=C groups: C=C(C)C(=O)OCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOC(=O)C(=C)C\n",
      "Monomer 2 includes CCS groups: O=c1n(CCS)c(=O)n(CCS)c(=O)n1CCS\n"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "def generate_new_TSMP(role,prompt_content, isFinalQuery=False):\n",
    "    propmt={\"role\":role, \"content\": prompt_content}\n",
    "    messages.append(propmt)\n",
    "    if not isFinalQuery:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=200,\n",
    "            n=2\n",
    "        )\n",
    "        result= completion\n",
    "       \n",
    "        \n",
    "   \n",
    "    messages.append({\"role\":'assistant', \"content\": result})\n",
    "    \n",
    "    # Print the response from the assistant\n",
    "    return result, messages\n",
    "\n",
    "replies, messages = generate_new_TSMP('user',group_specific_message[1], isFinalQuery=True)\n",
    "print(\"User: \",group_specific_message[1])\n",
    "print(\"Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \")\n",
    "for i in range(2):\n",
    "    print(\"Sample : :\",i+1, replies.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4dbdfbe-b881-4641-93fa-a839aa876022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Please generate some TSMP with Tg = 50C and Er= 100Mpa\n",
      "Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \n",
      "Sample : : 1 Here is a TSMP designed for Tg = 50C and Er = 100Mpa:\n",
      "Monomer 1: CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3CO3)cc4\n",
      "Monomer 2: CC(N)COCC(C)COCC(C)COCC(C)COCC(C)COCC(C)COCC(C)N\n",
      "Sample : : 2 To achieve Tg of 50C and Er of 100Mpa, consider this monomer combination:\n",
      "Monomer 1: C=CCc1ccccc1OCC(O)COC(=O)Cc3ccc(C(=O)OCC(O)COc2ccccc2CC=C)cc3\n",
      "Monomer 2: C=CCOCC(CO)(COCC=C)COCC=C\n"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "def generate_new_TSMP(role,prompt_content, isFinalQuery=False):\n",
    "    propmt={\"role\":role, \"content\": prompt_content}\n",
    "    messages.append(propmt)\n",
    "    if not isFinalQuery:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=200,\n",
    "            n=2\n",
    "        )\n",
    "        result= completion\n",
    "       \n",
    "        \n",
    "   \n",
    "    messages.append({\"role\":'assistant', \"content\": result})\n",
    "    \n",
    "    # Print the response from the assistant\n",
    "    return result, messages\n",
    "\n",
    "replies, messages = generate_new_TSMP('user',proeprty_specific_message[1], isFinalQuery=True)\n",
    "print(\"User: \",proeprty_specific_message[1])\n",
    "print(\"Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \")\n",
    "for i in range(2):\n",
    "    print(\"Sample : :\",i+1, replies.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83ea42d3-9c71-4c77-a841-f77d2e1b271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Please generate some TSMP with Tg = 50C and Er= 100Mpa and Thiol(CCS) groups in monomer 1 and vinyl(C=C) groups in monomer 2\n",
      "Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \n",
      "Sample : : 1 To meet the requirements of Tg = 100C and Er = 150Mpa, with C=C groups in monomers 1 and 2, I suggest this combination:\n",
      "Monomer 1: C=C(C)C(=C)OCOCCOC(=O)C(=C)C\n",
      "Monomer 2: C=C(C)C(=O)OCCOCCOCCOCCOCCOCCOCCOCCOCCOC(=O)C(=C)C\n",
      "Sample : : 2 To achieve Tg of 100C and Er of 150Mpa with monomers containing C=C groups, try:\n",
      "Monomer 1: C=C(C)C(=O)OCCOCCOCCOCCOCCOCCOCCOCCOC(=O)C(=C)C\n",
      "Monomer 2: C=C(C)C(=O)OCCOCCOCCOCCOCCOC(=O)C(C)=O\n"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "def generate_new_TSMP(role,prompt_content, isFinalQuery=False):\n",
    "    propmt={\"role\":role, \"content\": prompt_content}\n",
    "    messages.append(propmt)\n",
    "    if not isFinalQuery:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-mini-2024-07-18:personal::BKodpSOI',\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=200,\n",
    "            n=2\n",
    "        )\n",
    "        result= completion\n",
    "       \n",
    "        \n",
    "   \n",
    "    messages.append({\"role\":'assistant', \"content\": result})\n",
    "    \n",
    "    # Print the response from the assistant\n",
    "    return result, messages\n",
    "\n",
    "replies, messages = generate_new_TSMP('user',mixed_specific_message[0], isFinalQuery=True)\n",
    "print(\"User: \",mixed_specific_message[1])\n",
    "print(\"Assistant: Two TSMPs for you ( as I told model to generate two samples per query) \")\n",
    "for i in range(2):\n",
    "    print(\"Sample : :\",i+1, replies.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43d6b7-ea37-43b8-81b8-37f450ed179f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
